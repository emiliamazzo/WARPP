import openai
from typing import List


def get_formatted_client_prompt(df_row, prompt) -> str:
    """
    Generates a formatted client prompt by replacing placeholders in the provided prompt with relevant data 
    from the given row of data (df_row). The prompt is tailored based on client information such as their ID, 
    mobile phone number, authentication code, and other details available in the row.

    Args:
        df_row (dict): A dictionary containing the client's data, including ID, phone info, authentication code, 
                       and other relevant fields.
        prompt (str): A string containing placeholders ('INTENT', 'INFO', 'FIRST UTTERANCE', 'SPECIFIC TASK') 
                      to be replaced with corresponding values from df_row.

    Returns:
        str: The prompt with placeholders replaced by actual client information.
    """
    # handle different ID fields based on domain
    info_needed = df_row.get('user_provided_info', {})
    first_utt = info_needed.pop('first_utterance', '')

    replacements = {
        '{INTENT}': df_row['intent'],
        '{FIRST UTTERANCE}': f"***{first_utt}***",
        '{SPECIFIC TASK}': info_needed
    }

    for placeholder, value in replacements.items():
        prompt = prompt.replace(placeholder, str(value))
    return prompt


async def generate_llm_as_client_response(conversation_history: list[str], client_prompt) -> str:
    """
    Generates a response from an LLM by sending the conversation history and the formatted
    client prompt to the model. This function alternates between user and assistant roles.

    Args:
        conversation_history (list[str]): A list of messages representing the conversation history, where 
                                           each message contains a 'role' (either 'user' or 'assistant') 
                                           and 'content' (the actual message text).
        client_prompt (str): A string containing the initial prompt that will be used by the assistant to guide 
                             the conversation.

    Returns:
        str: The response from the assistant generated by the LLM, or an error message if the request fails.
    """
    try:
        local_history = conversation_history.copy()
        messages = [{"role": "system", "content": client_prompt}]
        
        # add each message from convo history
        for message in local_history:
            if 'content' in message:
                
                # assistant messages
                if message['role'] == 'assistant':
                    # check if list 
                    if isinstance(message['content'], list):
                        content = message['content'][0]['text']  
                    else:
                        content = message['content']  # if string, use it directly
                    messages.append({"role": "user", "content": content})

                
                # user messages
                if message['role'] == 'user':
                    content = message['content'] 
                    messages.append({"role": "assistant", "content": content})

        response = openai.chat.completions.create(
            model="gpt-4o",  
            messages=messages,
            max_tokens=150,
            temperature=0.7,
        )
            
        assistant_response = response.choices[0].message.content
        print('\033[33mClient LLM: \033[0m', assistant_response)
        return assistant_response

    except Exception as e:
        print(f'Error: {e}')
        return f"Error with LLM response: {e}"